{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries & Dataset Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact_lenses.csv\n",
      "covid_categorical_good.csv\n",
      "StudentEvaluations.csv\n",
      "titanic.csv\n",
      "two_houses.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "for list_of_files in os.listdir(\"../data_sets\"):\n",
    "\tprint(list_of_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns of the contact_lenses dataset:\n",
      " Index(['age', 'spectacles', 'astigmatism', 'tear production rate',\n",
      "       'lenses type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_file = \"contact_lenses.csv\"\n",
    "data = pd.read_csv(data_file, index_col=['id'])\n",
    "print(\"\\nColumns of the contact_lenses dataset:\\n\", data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes\n",
    "conditions = [ data['lenses type'].eq(1), data['lenses type'].eq(2), data['lenses type'].eq(3)]\n",
    "choices = [\"hard\",\"soft\",\"none\"]\n",
    "\n",
    "data['lenses type'] = np.select(conditions, choices)\n",
    "\n",
    "# age groups\n",
    "conditions = [ data['age'].eq(1), data['age'].eq(2), data['age'].eq(3)]\n",
    "choices = [\"young\",\"medium\",\"old\"]\n",
    "\n",
    "data['age'] = np.select(conditions, choices)\n",
    "\n",
    "# spectacles\n",
    "conditions = [ data['spectacles'].eq(1), data['spectacles'].eq(2)]\n",
    "choices = [\"nearsighted\",\"farsighted\"]\n",
    "\n",
    "data['spectacles'] = np.select(conditions, choices)\n",
    "\n",
    "# astigmatism\n",
    "conditions = [ data['astigmatism'].eq(1), data['astigmatism'].eq(2)]\n",
    "choices = [\"no\",\"yes\"]\n",
    "\n",
    "data['astigmatism'] = np.select(conditions, choices)\n",
    "\n",
    "# tear production rate\n",
    "conditions = [ data['tear production rate'].eq(1), data['tear production rate'].eq(2)]\n",
    "choices = [\"reduced\",\"normal\"]\n",
    "\n",
    "data['tear production rate'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule:\n",
    "    def __init__(self, class_label):\n",
    "        self.conditions = []  # list of conditions\n",
    "        self.class_label = class_label  # rule class\n",
    "        self.accuracy = 0\n",
    "        self.coverage = 0\n",
    "\n",
    "    def addCondition(self, condition):\n",
    "        self.conditions.append(condition)\n",
    "\n",
    "    def setParams(self, accuracy, coverage):\n",
    "        self.accuracy = accuracy\n",
    "        self.coverage = coverage\n",
    "    \n",
    "    # Human-readable printing of this Rule\n",
    "    def __repr__(self):\n",
    "        return \"If {} then {}. Coverage:{}, accuracy: {}\".format(self.conditions, self.class_label,\n",
    "                                                                 self.coverage, self.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Condition:\n",
    "    def __init__(self, attribute, value, true_false = None):\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.true_false = true_false\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.true_false is None:\n",
    "            return \"{}={}\".format(self.attribute, self.value)\n",
    "        else:\n",
    "            return \"{}>={}:{}\".format(self.attribute, self.value, self.true_false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn One Rule\n",
    "\n",
    "My first approach while developing \"learn_one_rule\" was to follow the instructions from the slides \"Classification Rules\" (slide 30) for the Prism Algorithm implementation. My first attempt was to impelement with loops, and the after roughly implementing the algorithm, I tried to improve it with list comprehensions. I also used Pandas DataFrame functions such as groupby, apply, and lambda for better performance. For code debbuging, I tried many times to play with every line of the code and previous functions that had be given to see what each variable was returning after every interation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_one_rule(columns, data, class_label, min_coverage = 30, min_accuracy = 0.6):\n",
    "    covered_subset = data.copy()\n",
    "\n",
    "    # YOUR CODE      \n",
    "    rule = Rule(class_label)\n",
    "    \n",
    "    # Creating a set for the available features.\n",
    "    features = set(columns[:-1]) #Not including the last column (class_label)\n",
    "    target = columns[-1]\n",
    "\n",
    "    # As long there is a feature, we then create a condition.\n",
    "    while features:\n",
    "\n",
    "        # The following best \"attributes\" are initialized to zero.\n",
    "        # Checking the accuracy for each attribute and its values.\n",
    "        best_accuracy = 0\n",
    "        best_feature = None\n",
    "        best_value = 0\n",
    "        best_coverage = 0\n",
    "\n",
    "        # This loop will run through each available feature.\n",
    "\n",
    "        for col in features:\n",
    "            \n",
    "            # Calculates acurracy/coverage for specific feature on at the time.\n",
    "            '''\n",
    "                Below we have some Pandas functions, where we start with the original dataset.\n",
    "                We \"groupby\" to a specific column and then we extract targe column which is class_label.\n",
    "                Then, we \"apply\" a feature. The \"apply\" function searches for what percentage of the feature is equals to class_label (accuracy).\n",
    "                It also checks for how many those we have (shape).\n",
    "\n",
    "                For example:\n",
    "                    In the case below, we have \"age\" as a feature (cur) and class_label as \"none\".\n",
    "                    So here, we can see that \"age\" has three cases being \"old\", \"medium\", and \"young\". \n",
    "                    We then can infer that 75% of the rows in \"age\" are \"old\" within the class_label as none. The percentage is the accuracy, and 8 represents the covarege.. \n",
    "                    \n",
    "                    age\n",
    "                    old        (0.75, 8)\n",
    "                    medium    (0.625, 8)\n",
    "                    young       (0.5, 8) \n",
    "\n",
    "                    The sorted values will give us the highest value among all accuracy since we don't care about the other ones. \n",
    "\n",
    "            ''' \n",
    "            cur = covered_subset.groupby(col)[target].apply(lambda x: ((x == class_label).mean(),x.shape[0])).sort_values(ascending=False)\n",
    "            cur = cur[cur.apply(lambda x: x[1] > min_coverage)]\n",
    "            if cur.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Selecting the best accuracy for the especific column.\n",
    "            cur = cur.head(1) \n",
    "\n",
    "            # This if statement keeps track of the best acurracy feature among features. \n",
    "            if best_accuracy < cur.values[0][0] or (cur.values[0][0] == best_accuracy and cur.values[0][1] > best_coverage):\n",
    "                best_feature = col\n",
    "                best_value = cur.index[0]\n",
    "                best_accuracy = cur.values[0][0]\n",
    "                best_coverage = cur.values[0][1]\n",
    "\n",
    "        if best_feature is None:\n",
    "            return None, None\n",
    "\n",
    "        # After selecting the a feature, we then remove it from the dataset so that we can avoid using it again in the next interation of the loop.\n",
    "        features.remove(best_feature)\n",
    "        # We then add the next condition to the rule to what is the best feature corresponding to the value. \n",
    "        rule.addCondition(Condition(best_feature, best_value))\n",
    "        # We then only select the data specified by the rule.\n",
    "        covered_subset = covered_subset[covered_subset[best_feature] == best_value].copy()\n",
    "        \n",
    "        # If the accuracy we achieve so far is higher than min_accuracy than we stopped.\n",
    "        # If not, we then go back to the while loop.\n",
    "        if best_accuracy > min_accuracy:\n",
    "            break\n",
    "\n",
    "    # We then achieve the minimal accuracy or there is no other feature to create the condition.\n",
    "    rule.setParams(best_accuracy, covered_subset.shape[0])   \n",
    "    #print(rule.coverage, rule.accuracy)\n",
    "\n",
    "    # Here we then check if the coverage and the minimal accuracy is mantained.\n",
    "    if (rule.coverage < min_coverage) or (rule.accuracy < min_accuracy): \n",
    "        rule = None \n",
    "        rule = None\n",
    "\n",
    "    return rule, covered_subset                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_rules (columns, data, classes=None, \n",
    "                 min_coverage = 30, min_accuracy = 0.6):\n",
    "    # List of final rules\n",
    "    rules = []\n",
    "    \n",
    "    # If list of classes of interest is not provided - it is extracted from the last column of data\n",
    "    if classes is not None:\n",
    "        class_labels = classes\n",
    "    else:\n",
    "        class_labels = data[columns[-1]].unique().tolist()\n",
    "\n",
    "    current_data = data.copy()\n",
    "    \n",
    "    # This follows the logic of the original PRISM algorithm\n",
    "    # It processes each class in turn. \n",
    "    for class_label in class_labels:\n",
    "        done = False\n",
    "        while len(current_data) > min_coverage and not done:\n",
    "            # Learn one rule \n",
    "            \n",
    "            rule, subset = learn_one_rule(columns, current_data, class_label, min_coverage, min_accuracy)\n",
    "            #print(current_data.shape, rule.accuracy)\n",
    "            # If the best rule does not pass the coverage threshold - we are done with this class\n",
    "            if rule is None:\n",
    "                break\n",
    "\n",
    "            # If we get the rule with accuracy and coverage above threshold\n",
    "            \n",
    "            if rule.accuracy >= min_accuracy:\n",
    "                rules.append(rule)\n",
    "\n",
    "                # remove rows covered by this rule\n",
    "                # you have to remove the rows where all of the conditions hold\n",
    "                current_data = current_data[~current_data.index.isin(subset.index)].copy()\n",
    "\n",
    "            else:\n",
    "                done = True         \n",
    "                \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 5)\n",
      "If [tear production rate=reduced] then none. Coverage:12, accuracy: 1.0\n",
      "If [astigmatism=no, spectacles=farsighted] then soft. Coverage:3, accuracy: 1.0\n",
      "If [astigmatism=yes, spectacles=nearsighted] then hard. Coverage:3, accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "column_list = data.columns.to_numpy().tolist()\n",
    "rules = learn_rules(column_list, data, None, 1, 0.95)\n",
    "for rule in rules[:20]:\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Results are given below for comparison:\n",
    "\n",
    "If [tear production rate=reduced] then none. Coverage:12, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, spectacles=farsighted] then soft. Coverage:3, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, age=young] then soft. Coverage:1, accuracy: 1.0\n",
    "\n",
    "If [astigmatism=no, age=medium] then soft. Coverage:1, accuracy: 1.0\n",
    "\n",
    "If [age=young] then hard. Coverage:2, accuracy: 1.0\n",
    "\n",
    "If [spectacles=nearsighted, astigmatism=yes] then hard. Coverage:2, accuracy: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While comparing the outputed results by the implemented algorithgm with the given results provided by Professor Barsky, it was possible to find similiraties and differences between them.\n",
    "\n",
    "Both results output similar rules in their first two printed lines:\n",
    "\n",
    "    If [tear production rate=reduced] then none. Coverage:12, accuracy: 1.0\n",
    "\n",
    "    If [astigmatism=no, spectacles=farsighted] then soft. Coverage:3, accuracy: 1.0\n",
    "\n",
    "However, their remaning printed lined are complemently different. Because the for loop \"class_label\" in the \"learn_rule\" function keeps using the same class unless the rule becomes empty, we have the immediate swith of classes from \"none\" to \"soft\", and \"hard\" being being in the total of tree outputed rules. It seems that after reaching each rule, we immedialty hit the best rule leading then to the next class label and finally finalizing with \"If [astigmatism=yes, spectacles=nearsighted] then hard. Coverage:3, accuracy: 1.0.\" At this point, we reach to stage where there is nothing better that we can do. \n",
    "\n",
    "For that reason, I then wondered if the coverage outputed in Barksy's given results where randomly printed since it feels that ther might have been other rules that could be shown in between one to another. \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8003219c8c57211ee3be347d121ba14ebad7276cdae3d94be72d9e4e17f9edd5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
